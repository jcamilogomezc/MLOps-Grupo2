services:
  mysql-mlflow-metadata:
    container_name: mysql_mlflow_metadata
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: mlflow_root_pass
      MYSQL_DATABASE: mlflow_metadata
      MYSQL_USER: mlflow_meta
      MYSQL_PASSWORD: mlflow_meta_pass
    command: ["--default-authentication-plugin=mysql_native_password"]
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h localhost -umlflow_meta -pmlflow_meta_pass --silent"]
      interval: 10s
      retries: 20
      start_period: 20s
    restart: always
    networks:
      - database
    volumes:
      - mysql_data:/var/lib/mysql

  mlflow:
    container_name: mlflow_server
    build: ./mlflow
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=supersecret
      - MLFLOW_S3_IGNORE_TLS=true
      - MLFLOW_BUCKET_NAME=mlflow-artifacts
    command: >
      server
      --backend-store-uri mysql+pymysql://mlflow_meta:mlflow_meta_pass@mysql-mlflow-metadata:3306/mlflow_metadata
      --artifacts-destination s3://mlflow-artifacts
      --serve-artifacts
      --host 0.0.0.0
      --port 5000
    ports:
      - "8005:5000"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: always
    depends_on:
      mysql-mlflow-metadata:
        condition: service_healthy
      minio:
        condition: service_healthy
      create-minio-buckets:
        condition: service_completed_successfully
    networks:
      - ml-services
      - database

  minio:
    container_name: minio_server
    image: quay.io/minio/minio:latest
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=supersecret
    command: server /data
    ports:
      - "8009:9000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10s
    restart: always
    networks:
      - ml-services
    volumes:
      - minio_data:/data

  create-minio-buckets:
    container_name: minio_bucket_creator
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      /usr/bin/mc alias set myminio http://minio:9000 admin supersecret;
      /usr/bin/mc mb -p myminio/mlflow-artifacts;
      exit 0;"
    networks:
      - ml-services

  model-trainer:
    build:
      context: ./inference_api
      dockerfile: Dockerfile.trainer
    container_name: model_trainer
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - REGISTERED_MODEL_NAME=linear_regression_model
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=supersecret
    depends_on:
      mlflow:
        condition: service_healthy
      mysql-mlflow-metadata:
        condition: service_healthy
    networks:
      - ml-services
    restart: "no"  # Only run once

  # OPTIMIZED: Inference API with performance tuning (scalable)
  fastapi-app:
    # build:
      # context: ./inference_api
      # dockerfile: Dockerfile.inference
    # image: germanaoq/inference-fast-api:v1.0
    # https://hub.docker.com/repository/docker/jcamilogomezc/t5_inference_api/general
    image: jcamilogomezc/t5_inference_api:v2.0
    # NOTE: No container_name to allow multiple replicas with --scale
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - REGISTERED_MODEL_NAME=linear_regression_model
      - MODEL_STAGE_OR_VERSION=Production
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=supersecret
    # NOTE: Use expose instead of ports - nginx will access internally
    expose:
      - "8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    restart: always
    deploy:
      resources:
        limits:
          cpus: '2.0'  # Per replica limit
          memory: 4G
        reservations:
          cpus: '1.5'
          memory: 2G
    depends_on:
      model-trainer:
        condition: service_completed_successfully
      mlflow:
        condition: service_healthy
    networks:
      - ml-services
    # CRITICAL: Use shared memory for worker communication
    shm_size: '1gb'
    tmpfs:
      - /dev/shm:size=1G
    # CRITICAL: Increase ulimits for high connections
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      nproc:
        soft: 32768
        hard: 32768

  # Nginx Load Balancer - distributes traffic to fastapi-app replicas
  nginx-lb:
    image: nginx:alpine
    container_name: nginx_load_balancer
    ports:
      - "8000:80"  # Single external port exposed for load testing
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - fastapi-app
    restart: always
    networks:
      - ml-services
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3

networks:
  database:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: app-database
    ipam:
      config:
        - subnet: 172.22.0.0/16

  ml-services:
    driver: bridge

volumes:
  mysql_data:
  minio_data: