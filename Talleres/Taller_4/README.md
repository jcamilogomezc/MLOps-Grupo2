# Taller 4: Nivel 2 - Desarrollo con MLflow y todo un Ecosistema MLOps con MinIO, JupyterLab y FastAPI

## Integrantes
* Edgar Cruz Martinez
* Juan Camilo Gomez Cano
* Germ√°n Andr√©s Ospina Quintero

## Documentaci√≥n del funcionamiento

En el siguiente video se presenta el funcionamiento del proyecto:

[![Mira el video en YouTube](https://img.youtube.com/vi/XRCuq-75cLA/0.jpg)](https://www.youtube.com/watch?v=XRCuq-75cLA)

‚∏ª


Este repo levanta un entorno local estilo ‚Äúnube‚Äù para todo el ciclo de vida de ML:
**experimentaci√≥n ‚Üí tracking/artefactos ‚Üí registro de modelos ‚Üí inferencia en API**.

---

## üß± Componentes

| Servicio         | Host:Puerto ‚Üí Contenedor | Rol |
|------------------|--------------------------|-----|
| **MLflow**       | `8080 ‚Üí 5000`            | Tracking + Model Registry |
| **MinIO (S3)**   | `8001 ‚Üí 9000` (API), `8002 ‚Üí 9001` (UI) | Artefactos de modelos |
| **Postgres meta**| interno: `5432`          | Metadatos de MLflow |
| **Postgres main**| interno: `5432`          |  datasets |
| **JupyterLab**   | `8003 ‚Üí 8888`            | Notebooks y experimentos |
| **FastAPI**      | `8013 ‚Üí 8013`            | Inferencia del modelo ‚Äúen producci√≥n‚Äù |

> Las URLs de ejemplo asumen ejecuci√≥n en **localhost**. Si vas por VPN o IP remota, reemplaza `localhost` por la IP correspondiente.

---

## ‚úÖ Prerrequisitos

- Docker + Docker Compose
- (Opcional) `make` para usar los atajos del **Makefile**
- (Opcional) `curl` o Postman para probar la API

---

## üöÄ Quick start

### 1) Clonar y entrar al proyecto
```bash
git clone https://github.com/jcamilogomezc/MLOps-Grupo2.git
cd MLOps-Grupo2/Talleres/Taller_4

2) Arrancar todo (con build)

docker compose up -d --build

o con make:

make up

Verifica:

docker ps

3) Abrir servicios
	‚Ä¢	MLflow UI: http://localhost:8080
	‚Ä¢	MinIO UI: http://localhost:8002
	‚Ä¢	JupyterLab: http://localhost:8003
	‚Ä¢	token: configurado en docker-compose.yml (o corre jupyter server list dentro del contenedor)
	‚Ä¢	API (docs): http://localhost:8013/docs

‚∏ª

üß™ Entrenamiento y registro en MLflow
	1.	Entra a JupyterLab ‚Üí abre jupyter/notebooks/Pinguinos.ipynb.
	2.	Ejecuta las celdas. El notebook:
	‚Ä¢	Lee datos (y puede escribir a BD si lo ajustas).
	‚Ä¢	Realiza ‚â•20 corridas variando hiperpar√°metros.
	‚Ä¢	Loguea par√°metros, m√©tricas y artefactos en MLflow.
	3.	Abre MLflow UI (http://localhost:8080), revisa el experimento.
	4.	Registra el mejor modelo como PenguinsClassifier y promu√©velo a Stage = Production.

Importante: La API solo carga el modelo si hay una versi√≥n en Production (o si defines una versi√≥n expl√≠cita por variable de entorno).

‚∏ª

üåê Probar la API de inferencia

Si ya marcaste un modelo en Production, reinicia la API:

docker compose restart api_inference
# o
make restart-api

Healthcheck

curl http://localhost:8013/health

Predicci√≥n

curl -X POST http://localhost:8013/predict \
  -H "Content-Type: application/json" \
  -d '{
    "island": "Torgersen",
    "sex": "male",
    "bill_length_mm": 39.1,
    "bill_depth_mm": 18.7,
    "flipper_length_mm": 181,
    "body_mass_g": 3750
  }'

Respuesta esperada (ejemplo):

{
  "species": "Adelie",
  "probabilities": {
    "Adelie": 0.92,
    "Chinstrap": 0.05,
    "Gentoo": 0.03
  }
}

Si te devuelve "probabilities": {}, tu modelo no expone predict_proba. Puedes ajustar la API para mapear manualmente o registrar un wrapper pyfunc.

‚∏ª

‚öôÔ∏è Variables de entorno clave

La API lee estas variables (ver docker-compose.yml):
	‚Ä¢	MLFLOW_TRACKING_URI ‚Üí http://mlflow:5000
	‚Ä¢	REGISTERED_MODEL_NAME ‚Üí PenguinsClassifier
	‚Ä¢	MODEL_STAGE_OR_VERSION ‚Üí Production  (o un n√∫mero de versi√≥n, p. ej. 1)
	‚Ä¢	MLFLOW_S3_ENDPOINT_URL ‚Üí http://minio:9000
	‚Ä¢	AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY ‚Üí credenciales de MinIO

Puedes centralizar en .env (ver ejemplo m√°s abajo).

‚∏ª

üß≠ Salud de servicios (healthchecks)
	‚Ä¢	MLflow: simplificado a un GET http://localhost:5000 dentro del contenedor.
	‚Ä¢	MinIO: health endpoint http://localhost:9000/minio/health/live.

Si un contenedor queda unhealthy, simplifica el healthcheck o aumenta interval/timeout/retries.

‚∏ª

üß± Barreras (lo que ya nos pas√≥ y c√≥mo lo arreglamos)
	1.	Puertos bloqueados / firewall
	‚Ä¢	Escaneamos con: nc -zv <IP> 8000-8100
	‚Ä¢	Puertos que usamos y normalmente est√°n bien: 8080, 8001, 8002, 8003, 8013
	2.	Builds fallando por contexto
	‚Ä¢	Error cl√°sico: COPY requirements.txt not found
	‚Ä¢	Soluci√≥n: cada servicio tiene su propia carpeta y el build.context apunta all√≠.
	3.	API no encuentra modelo
	‚Ä¢	Error: No versions of model 'PenguinsClassifier' in stage 'Production' found
	‚Ä¢	Soluci√≥n: registra el modelo desde MLflow UI y mu√©velo a Production. Luego restart de la API.
	4.	Token Jupyter
	‚Ä¢	Fija JUPYTER_TOKEN en compose o corre jupyter server list dentro del contenedor.
	5.	Probabilidades vac√≠as
	‚Ä¢	Algunos clasificadores no soportan predict_proba. Ajusta el pipeline/modelo o la API para manejarlo.

‚∏ª

üñºÔ∏è Arquitectura

flowchart LR
    subgraph Usuario
        A[Jupyter Notebook] -->|Entrena/Loguea| B[MLflow]
        C[curl / Cliente API] -->|Predicci√≥n| E[FastAPI]
    end

    subgraph Infraestructura
        B[MLflow] -->|Metadatos| D[(Postgres Meta)]
        B -->|Artefactos| F[(MinIO - S3)]
        E[FastAPI] -->|Carga modelo| B
        J[JupyterLab] -->|Experimentaci√≥n| B
    end





